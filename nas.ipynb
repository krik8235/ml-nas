{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56baf734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running all three search strategies for comparison ---\n",
      "Starting Evolutionary Search with a population of 10 for 5 generations...\n",
      "\n",
      "--- Generation 1/5 ---\n",
      "  Best loss in this generation: 0.3794\n",
      "\n",
      "--- Generation 2/5 ---\n",
      "  Best loss in this generation: 0.3080\n",
      "\n",
      "--- Generation 3/5 ---\n",
      "  Best loss in this generation: 0.3199\n",
      "\n",
      "--- Generation 4/5 ---\n",
      "  Best loss in this generation: 0.3684\n",
      "\n",
      "--- Generation 5/5 ---\n",
      "  Best loss in this generation: 0.2911\n",
      "\n",
      "--- Evolutionary Search Complete ---\n",
      "Best Architecture Found:\n",
      "  Num Hidden Layers: 4\n",
      "  Hidden Layer Size: 32\n",
      "  Activation Function: ReLU\n",
      "  Learning Rate: 0.001\n",
      "  Optimizer: SGD\n",
      "  Dropout Rate: 0.6\n",
      "Best Validation MSE: 0.2911\n",
      "\n",
      "==================================================\n",
      "\n",
      "Starting RL Search with 5 episodes...\n",
      "  Episode 1: Loss = 1.3425, Reward = -1.3425\n",
      "  Episode 2: Loss = 0.9457, Reward = -0.9457\n",
      "  Episode 3: Loss = 0.5463, Reward = -0.5463\n",
      "  Episode 4: Loss = 1.0160, Reward = -1.0160\n",
      "  Episode 5: Loss = 0.7519, Reward = -0.7519\n",
      "\n",
      "--- RL Search Complete ---\n",
      "Best Architecture Found:\n",
      "  Num Hidden Layers: 4\n",
      "  Hidden Layer Size: 32\n",
      "  Activation Function: ReLU\n",
      "  Learning Rate: 0.0001\n",
      "  Optimizer: SGD\n",
      "  Dropout Rate: 0.4\n",
      "Best Validation MSE: 0.5463\n",
      "\n",
      "==================================================\n",
      "\n",
      "Starting Gradient-based Search with 50 epochs...\n",
      "  Epoch 10/50: Train Loss = 0.2446, Arch Loss = 2.1230\n",
      "  Epoch 20/50: Train Loss = 0.0576, Arch Loss = 1.2760\n",
      "  Epoch 30/50: Train Loss = 0.0318, Arch Loss = 1.6483\n",
      "  Epoch 40/50: Train Loss = 0.0156, Arch Loss = 0.8292\n",
      "  Epoch 50/50: Train Loss = 0.0126, Arch Loss = 0.3733\n",
      "\n",
      "--- Gradient-based Search Complete ---\n",
      "Found architecture from continuous search:\n",
      "{'num_hidden_layers': 5, 'hidden_layer_size': 32, 'learning_rate': 0.001, 'optimizer': 'Adam', 'dropout_rate': 0.0, 'activation_function': 'LeakyReLU'}\n",
      "\n",
      "--- Gradient-based Search Complete ---\n",
      "Best Architecture Found:\n",
      "  Num Hidden Layers: 5\n",
      "  Hidden Layer Size: 32\n",
      "  Learning Rate: 0.001\n",
      "  Optimizer: Adam\n",
      "  Dropout Rate: 0.0\n",
      "  Activation Function: LeakyReLU\n",
      "Best Validation MSE: 3.9948\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "search_space = {\n",
    "    'num_hidden_layers': [1, 2, 3, 4, 5],\n",
    "    'hidden_layer_size': [32, 64, 128, 256, 512],\n",
    "    'activation_function': ['ReLU', 'LeakyReLU', 'Tanh'],\n",
    "    'learning_rate': [0.1, 0.01, 0.001, 0.0001],\n",
    "    'optimizer': ['Adam', 'SGD', 'RMSprop'],\n",
    "    'dropout_rate': [0.0, 0.2, 0.4, 0.6]\n",
    "}\n",
    "\n",
    "activation_map = {\n",
    "    'ReLU': nn.ReLU,\n",
    "    'LeakyReLU': nn.LeakyReLU,\n",
    "    'Tanh': nn.Tanh\n",
    "}\n",
    "\n",
    "optimizer_map = {\n",
    "    'Adam': optim.Adam,\n",
    "    'SGD': optim.SGD,\n",
    "    'RMSprop': optim.RMSprop\n",
    "}\n",
    "\n",
    "\n",
    "def create_dataset(num_samples=1000):\n",
    "    x = torch.linspace(-5, 5, num_samples).unsqueeze(1)\n",
    "    y = torch.sin(x) + 0.1 * torch.randn(x.size())\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def build_model(architecture):\n",
    "    layers = []\n",
    "    layers.append(nn.Linear(1, architecture['hidden_layer_size']))\n",
    "    layers.append(activation_map[architecture['activation_function']]())\n",
    "    \n",
    "    for _ in range(architecture['num_hidden_layers'] - 1):\n",
    "        layers.append(nn.Linear(architecture['hidden_layer_size'], architecture['hidden_layer_size']))\n",
    "        layers.append(activation_map[architecture['activation_function']]())\n",
    "        layers.append(nn.Dropout(p=architecture['dropout_rate']))\n",
    "\n",
    "    layers.append(nn.Linear(architecture['hidden_layer_size'], 1))\n",
    "\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def evaluate_architecture(architecture, X_train, y_train, X_val, y_val, num_epochs=50):\n",
    "    model = build_model(architecture)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    optimizer_class = optimizer_map[architecture['optimizer']]\n",
    "    optimizer = optimizer_class(model.parameters(), lr=architecture['learning_rate'])\n",
    "    \n",
    "    model.train()\n",
    "    for _ in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val)\n",
    "        val_loss = criterion(val_outputs, y_val)\n",
    "    \n",
    "    return val_loss.item()\n",
    "\n",
    "\n",
    "def run_evolutionary_search(search_space, population_size=10, num_generations=5):\n",
    "    best_loss = float('inf')\n",
    "    best_architecture = None\n",
    "    \n",
    "    X, y = create_dataset()\n",
    "    split_idx = int(len(X) * 0.8)\n",
    "    X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "    \n",
    "    print(f\"Starting Evolutionary Search with a population of {population_size} for {num_generations} generations...\")\n",
    "    \n",
    "    population = []\n",
    "    for _ in range(population_size):\n",
    "        architecture = {key: random.choice(search_space[key]) for key in search_space}\n",
    "        population.append(architecture)\n",
    "\n",
    "    for gen in range(num_generations):\n",
    "        print(f\"\\n--- Generation {gen+1}/{num_generations} ---\")\n",
    "        \n",
    "        fitness = []\n",
    "        for arch in population:\n",
    "            loss = evaluate_architecture(arch, X_train, y_train, X_val, y_val, num_epochs=10)\n",
    "            fitness.append((loss, arch))\n",
    "            \n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_architecture = arch\n",
    "        \n",
    "        fitness.sort(key=lambda x: x[0])\n",
    "        print(f\"  Best loss in this generation: {fitness[0][0]:.4f}\")\n",
    "        \n",
    "        new_population = []\n",
    "        num_elites = population_size // 2\n",
    "        elites = [arch for loss, arch in fitness[:num_elites]]\n",
    "        new_population.extend(elites)\n",
    "        \n",
    "        while len(new_population) < population_size:\n",
    "            parent1 = random.choice(elites)\n",
    "            parent2 = random.choice(elites)\n",
    "            \n",
    "            child = deepcopy({})\n",
    "            for key in parent1:\n",
    "                child[key] = random.choice([parent1[key], parent2[key]])\n",
    "            \n",
    "            mutation_key = random.choice(list(search_space.keys()))\n",
    "            child[mutation_key] = random.choice(search_space[mutation_key])\n",
    "            \n",
    "            new_population.append(child)\n",
    "        \n",
    "        population = new_population\n",
    "\n",
    "    return best_architecture, best_loss\n",
    "\n",
    "\n",
    "class ArchitectureController(nn.Module):\n",
    "    def __init__(self, search_space):\n",
    "        super(ArchitectureController, self).__init__()\n",
    "        self.search_space = search_space\n",
    "        self.keys = list(search_space.keys())\n",
    "        self.vocab_size = [len(search_space[key]) for key in self.keys]\n",
    "        self.num_actions = len(self.keys)\n",
    "        self.rnn = nn.RNN(input_size=1, hidden_size=64, num_layers=1)\n",
    "        self.policy_heads = nn.ModuleList([nn.Linear(64, vs) for vs in self.vocab_size])\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output, hidden = self.rnn(input, hidden)\n",
    "        logits = [head(output.squeeze(0)) for head in self.policy_heads]\n",
    "        return logits, hidden\n",
    "\n",
    "   \n",
    "def run_rl_search(search_space, X_train, y_train, X_val, y_val, num_epochs=10, num_episodes=5):\n",
    "    controller = ArchitectureController(search_space)\n",
    "    controller_optimizer = optim.Adam(controller.parameters(), lr=0.01)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    best_architecture = None\n",
    "    \n",
    "    print(f\"Starting RL Search with {num_episodes} episodes...\")\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        controller_optimizer.zero_grad()\n",
    "        hidden = torch.zeros(1, 1, 64)\n",
    "        log_probs = []\n",
    "        architecture = {}\n",
    "\n",
    "        for i, key in enumerate(controller.keys):\n",
    "            logits, hidden = controller(torch.zeros(1, 1, 1), hidden)\n",
    "            dist = torch.distributions.Categorical(logits=logits[i])\n",
    "            action_index = dist.sample()\n",
    "            \n",
    "            architecture[key] = search_space[key][action_index.item()]\n",
    "            log_probs.append(dist.log_prob(action_index))\n",
    "        \n",
    "        val_loss = evaluate_architecture(architecture, X_train, y_train, X_val, y_val, num_epochs=num_epochs)\n",
    "        \n",
    "        reward = -val_loss\n",
    "        policy_loss = torch.sum(torch.stack(log_probs) * -reward)\n",
    "        policy_loss.backward()\n",
    "        controller_optimizer.step()\n",
    "        \n",
    "        print(f\"  Episode {episode+1}: Loss = {val_loss:.4f}, Reward = {reward:.4f}\")\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_architecture = architecture\n",
    "\n",
    "    return best_architecture, best_loss\n",
    "\n",
    "\n",
    "class DifferentiableCell(nn.Module):\n",
    "    def __init__(self, in_features, out_features, ops):\n",
    "        super(DifferentiableCell, self).__init__()\n",
    "        self.ops = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(in_features, out_features), op()) for op in ops\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x, weights):\n",
    "        return sum(w * op(x) for w, op in zip(weights, self.ops))\n",
    "\n",
    "class DifferentiableModel(nn.Module):\n",
    "    def __init__(self, search_space):\n",
    "        super(DifferentiableModel, self).__init__()\n",
    "        \n",
    "        self.ops_list = [activation_map[name] for name in search_space['activation_function']]\n",
    "        self.num_ops = len(self.ops_list)\n",
    "        \n",
    "        self.num_hidden_layers = max(search_space['num_hidden_layers'])\n",
    "        self.hidden_layer_size = search_space['hidden_layer_size'][0]\n",
    "        \n",
    "        self.alphas = nn.Parameter(torch.randn(self.num_hidden_layers, self.num_ops, requires_grad=True))\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(1, self.hidden_layer_size))\n",
    "        for _ in range(self.num_hidden_layers - 1):\n",
    "            self.layers.append(DifferentiableCell(self.hidden_layer_size, self.hidden_layer_size, self.ops_list))\n",
    "        self.output_layer = nn.Linear(self.hidden_layer_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        architecture_weights = nn.functional.softmax(self.alphas, dim=-1)\n",
    "        \n",
    "        output = x\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                output = layer(output)\n",
    "            elif isinstance(layer, DifferentiableCell):\n",
    "                output = layer(output, architecture_weights[i-1])\n",
    "\n",
    "        return self.output_layer(output)\n",
    "\n",
    "    def discretize(self):\n",
    "        architecture = {\n",
    "            'num_hidden_layers': self.num_hidden_layers,\n",
    "            'hidden_layer_size': self.hidden_layer_size,\n",
    "            'learning_rate': 0.001,\n",
    "            'optimizer': 'Adam',\n",
    "            'dropout_rate': 0.0\n",
    "        }\n",
    "        \n",
    "        best_op_indices = self.alphas.argmax(dim=-1)\n",
    "        best_ops = [self.ops_list[i].__name__ for i in best_op_indices]\n",
    "        \n",
    "        architecture['activation_function'] = best_ops[0]\n",
    "        return architecture\n",
    "\n",
    "def run_gradient_based_search(search_space, X_train, y_train, X_val, y_val, num_epochs=50):\n",
    "    model = DifferentiableModel(search_space)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    arch_params = [model.alphas]\n",
    "    arch_param_ids = {id(p) for p in arch_params}\n",
    "    weight_params = [p for p in model.parameters() if p.requires_grad and id(p) not in arch_param_ids]\n",
    "    \n",
    "    optimizer_w = optim.Adam(weight_params, lr=0.01)\n",
    "    optimizer_alpha = optim.Adam(arch_params, lr=0.001)\n",
    "\n",
    "    print(f\"Starting Gradient-based Search with {num_epochs} epochs...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer_w.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss_w = criterion(outputs, y_train)\n",
    "        loss_w.backward()\n",
    "        optimizer_w.step()\n",
    "        \n",
    "        optimizer_alpha.zero_grad()\n",
    "        val_outputs = model(X_val)\n",
    "        loss_alpha = criterion(val_outputs, y_val)\n",
    "        loss_alpha.backward()\n",
    "        optimizer_alpha.step()\n",
    "        \n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f\"  Epoch {epoch+1}/{num_epochs}: Train Loss = {loss_w.item():.4f}, Arch Loss = {loss_alpha.item():.4f}\")\n",
    "\n",
    "    best_architecture = model.discretize()\n",
    "    \n",
    "    print(\"\\n--- Gradient-based Search Complete ---\")\n",
    "    print(\"Found architecture from continuous search:\")\n",
    "    print(best_architecture)\n",
    "    \n",
    "    final_model = build_model(best_architecture)\n",
    "    final_loss = evaluate_architecture(best_architecture, X_train, y_train, X_val, y_val, num_epochs=50)\n",
    "    \n",
    "    return best_architecture, final_loss\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X, y = create_dataset()\n",
    "    split_idx = int(len(X) * 0.8)\n",
    "    X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "\n",
    "    print(\"--- Running all three search strategies for comparison ---\")\n",
    "    \n",
    "    best_arch_ea, best_perf_ea = run_evolutionary_search(search_space, population_size=10, num_generations=5)\n",
    "    print(\"\\n--- Evolutionary Search Complete ---\")\n",
    "    print(\"Best Architecture Found:\")\n",
    "    for key, value in best_arch_ea.items(): # type: ignore\n",
    "        print(f\"  {key.replace('_', ' ').title()}: {value}\")\n",
    "    print(f\"Best Validation MSE: {best_perf_ea:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "    best_arch_rl, best_perf_rl = run_rl_search(search_space, X_train, y_train, X_val, y_val, num_episodes=5)\n",
    "    print(\"\\n--- RL Search Complete ---\")\n",
    "    print(\"Best Architecture Found:\")\n",
    "    for key, value in best_arch_rl.items(): # type: ignore\n",
    "        print(f\"  {key.replace('_', ' ').title()}: {value}\")\n",
    "    print(f\"Best Validation MSE: {best_perf_rl:.4f}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "    best_arch_gb, best_perf_gb = run_gradient_based_search(search_space, X_train, y_train, X_val, y_val)\n",
    "    print(\"\\n--- Gradient-based Search Complete ---\")\n",
    "    print(\"Best Architecture Found:\")\n",
    "    for key, value in best_arch_gb.items():\n",
    "        print(f\"  {key.replace('_', ' ').title()}: {value}\")\n",
    "    print(f\"Best Validation MSE: {best_perf_gb:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
